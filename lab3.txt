Модели трансформеры сейчас очень популярно. Но количество параметров слишком много и обучение становится дорогим. Что если попробовать уменьшить количество параметров разными видами разложения матриц внимания. Например сингулярное разложение. 
Этап 1. Скачать разных популярных моделей (llama, mistral, qwen) с разным количеством параметров 1b, 3b, 8b.
Этап 2. Посчитать ранги матриц внимания по слоям.
Этап 3. Посмотреть зависит ли ранг от слоя.
Этап 4. Разложить матрицу внимания.
Этап 5. Попробовать обучить трансформер на низкоранговой модели.
Решение может помочь сэкономить много параметров и VRAM.